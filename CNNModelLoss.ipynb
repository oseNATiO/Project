{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import models, datasets, transforms\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define image transformations (including normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load training and test data\n",
    "full_train_data = datasets.MNIST(root=\".\\MNIST\\MNIST_Train\", train=True, download=False, transform = transform)\n",
    "test_data = datasets.MNIST(root=\".\\MNIST\\MNIST_Test\", train=False, download=False, transform= transform)\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(42)  # You can choose any seed number\n",
    "\n",
    "# Define train-validation split sizes\n",
    "train_size = int(0.8 * len(full_train_data))  # 80% for training\n",
    "val_size = len(full_train_data) - train_size  # 20% for validation\n",
    "\n",
    "# Split the full training dataset\n",
    "train_data, val_data = random_split(full_train_data, [train_size, val_size])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module): #Encoder as we're only interest in the final embedding provided by the CNN \n",
    "    def __init__(self):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.fc = nn.Linear(256 * 7 * 7, 128)  # Output: 128-dimension embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)  # 14x14\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)  # 7x7\n",
    "        \n",
    "        x = x.view(-1, 256 * 7 * 7)  # Flatten\n",
    "        x = self.fc(x)  # Output embeddings\n",
    "\n",
    "        # Normalize embeddings to lie on unit hypersphere\n",
    "        x = F.normalize(x, p=2, dim=1) # L2 normalization\n",
    "\n",
    "        return x\n",
    "\n",
    "class MLPClassifier(nn.Module): # MLP projection Head \n",
    "    def __init__(self):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, encoder, classifier, optimizer):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)  # First pass through the encoder to get the embeddings\n",
    "        x = self.classifier(x)  # Then pass the embeddings through the classifier\n",
    "        return x\n",
    "\n",
    "    def train_step(self, x, y):\n",
    "        \"\"\"Performs one training step: forward pass, loss calculation, backpropagation, and optimization\"\"\"\n",
    "        self.optimizer.zero_grad()  # Clear gradients\n",
    "        output = self.forward(x)    # Forward pass\n",
    "        loss = self.loss_fn(output, y)  # Calculate loss\n",
    "        loss.backward()             # Backpropagation\n",
    "        self.optimizer.step()       # Update the model parameters\n",
    "        return loss.item()          # Return the loss value for monitoring\n",
    "\n",
    "    def test_step(self, x, y):\n",
    "        \"\"\"Performs one testing step: forward pass and loss calculation\"\"\"\n",
    "        with torch.no_grad():  # Disable gradient calculation for testing/evaluation\n",
    "            output = self.forward(x)  # Forward pass\n",
    "            loss = self.loss_fn(output, y)  # Calculate loss\n",
    "            predicted = torch.argmax(output, dim=1)  # Get predicted class\n",
    "            correct = (predicted == y).sum().item()  # Calculate the number of correct predictions\n",
    "            total = y.size(0)  # Total number of samples\n",
    "        return loss.item(), correct, total  # Return loss, correct predictions, and total samples\n",
    "\n",
    "    def train_model(self, train_loader, epochs):\n",
    "        \"\"\"Train the model for a number of epochs\"\"\"\n",
    "        self.train()  # Set model to training mode (this does NOT call this method itself)\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch in train_loader:\n",
    "                x, y = batch\n",
    "                loss = self.train_step(x, y)  # Perform training step\n",
    "                total_loss += loss\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader)}\")  # Print average loss per epoch\n",
    "\n",
    "    def test_model(self, test_loader):\n",
    "        \"\"\"Evaluate the model on the test set\"\"\"\n",
    "        self.eval()  # Set model to evaluation mode\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        total_loss = 0\n",
    "        for batch in test_loader:\n",
    "            x, y = batch\n",
    "            loss, correct, total = self.test_step(x, y)  # Perform testing step\n",
    "            total_loss += loss\n",
    "            total_correct += correct\n",
    "            total_samples += total\n",
    "        accuracy = total_correct / total_samples  # Calculate accuracy\n",
    "        print(f\"Test Loss: {total_loss / len(test_loader)}, Accuracy: {accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 256, step=16)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    encoder = CNNEncoder().to(device)\n",
    "    classifier = MLPClassifier().to(device)\n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(classifier.parameters()), lr=lr)\n",
    "    \n",
    "    trainer = CNNModel(encoder, classifier, optimizer)\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train_model(train_loader, epochs=5)  # Use train_model instead of train\n",
    "    \n",
    "    # Evaluate on validation data\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            features = encoder(images)\n",
    "            outputs = classifier(features)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = CNNEncoder()\n",
    "#summary(model, input_size=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-19 11:34:56,736] A new study created in memory with name: no-name-dab2167e-c5f3-4f1b-95b5-bc424cacdeda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 104.47426743268967\n",
      "Epoch 2/5, Loss: 2.311341563463211\n",
      "Epoch 3/5, Loss: 2.3030362951755525\n",
      "Epoch 4/5, Loss: 2.3025848984718325\n",
      "Epoch 5/5, Loss: 2.302987458705902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-19 11:51:27,335] Trial 0 finished with value: 0.10375 and parameters: {'lr': 0.047002445658177475, 'batch_size': 240}. Best is trial 0 with value: 0.10375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.18304702355293556\n",
      "Epoch 2/5, Loss: 0.04610958827707994\n",
      "Epoch 3/5, Loss: 0.030378331612591864\n",
      "Epoch 4/5, Loss: 0.024228640268265735\n",
      "Epoch 5/5, Loss: 0.019762730693400954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-19 12:07:58,394] Trial 1 finished with value: 0.9896666666666667 and parameters: {'lr': 0.0009342924619966188, 'batch_size': 80}. Best is trial 1 with value: 0.9896666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'lr': 0.0009342924619966188, 'batch_size': 80}\n"
     ]
    }
   ],
   "source": [
    "# Run Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=2)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best hyperparameters: \", study.best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
