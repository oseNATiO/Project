{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Approach: CNN + MLP (Pre-trained with Supervised Contrastive Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import All libraries \n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import optuna\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_metric_learning.losses import SupConLoss\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perfom the transformations necessary \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                     #Transform the image into a tensor \n",
    "    transforms.Normalize((0.1307,), (0.3081,)) #Normalize the data\n",
    "])\n",
    "\n",
    "# Load the MNIST Dataset\n",
    "full_train_data = datasets.MNIST(root=\"D:\\MNIST\\MNIST_Train\", train=True, download=False, transform = transform)\n",
    "test_data = datasets.MNIST(root=\"D:\\MNIST\\MNIST_Test\", train=False, download=False, transform= transform)\n",
    "\n",
    "# This seed is used to make sure the same split will happen for all approaches\n",
    "torch.manual_seed(42)  \n",
    "\n",
    "# Defining the sets that are going to be used for training and Gridsearch\n",
    "train_data = int(0.7 * len(full_train_data))  # 70% for training\n",
    "train_gridsearch = len(full_train_data) - train_data # 30% for gridsearch\n",
    "\n",
    "# Get the training sets \n",
    "train_data, train_gridsearch = random_split(full_train_data, [train_data, train_gridsearch])\n",
    "\n",
    "# Split the training datasets, one for the training of the CNN and one for the training of the MLP \n",
    "train_CNN = int(0.6 * len(train_data))\n",
    "train_MLP = len(train_data) - train_CNN\n",
    "train_CNN, train_MLP = random_split(train_data, [train_CNN, train_MLP])\n",
    "\n",
    "# Make the Training and Validation split of the train_gridsearch set\n",
    "train_gridsearch2 = int(0.8 * len(train_gridsearch))\n",
    "validation_gridsearch = len(train_gridsearch) - train_gridsearch2\n",
    "train_gridsearch2, validation_gridsearch = random_split(train_gridsearch, [train_gridsearch2, validation_gridsearch])\n",
    "\n",
    "# Make the split of the train_gridsearch2 for the CNN and the MLP\n",
    "train_gridsearch2_CNN = int(0.6 * len(train_gridsearch2))\n",
    "train_gridsearch2_MLP = len(train_gridsearch2) - train_gridsearch2_CNN\n",
    "train_gridsearch2_CNN, train_gridsearch2_MLP = random_split(train_gridsearch2, [train_gridsearch2_CNN, train_gridsearch2_MLP])\n",
    "\n",
    "# the device specifies where the tensor or model should be stored and computations should be performed\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module): #Encoder as we're only interest in the final embedding provided by the CNN \n",
    "    def __init__(self):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(p=0.3) #Dropout\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(256 * 7 * 7, 128)  # Output: 128-dimension embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)  # 14x14\n",
    "        \n",
    "        x = self.dropout1(F.relu(self.conv3(x))) #Adding Dropout\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)  # 7x7\n",
    "        \n",
    "        x = x.view(-1, 256 * 7 * 7)  # Flatten\n",
    "        x = self.fc(x)  # Output embeddings\n",
    "        \n",
    "         # Normalize embeddings to lie on unit hypersphere\n",
    "        x = F.normalize(x, p=2, dim=1)  # L2 normalization\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MLPClassifier(nn.Module): # MLP projection Head \n",
    "    def __init__(self):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.dropout2 = nn.Dropout(p=0.3) #Dropout\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout2(F.relu(self.fc1(x))) #Adding Dropout\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize accuracy calculator\n",
    "#accuracy_calculator = AccuracyCalculator(include=(\"precision_at_1\",), knn_func=None)\n",
    "\n",
    "class ContrastiveTrainer:\n",
    "    def __init__(self, encoder, classifier, criterion, optimizer):\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train_contrastive(self, train_loader, epochs):\n",
    "        self.encoder.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass through the encoder\n",
    "                features = self.encoder(images)\n",
    "\n",
    "                # Compute supervised contrastive loss\n",
    "                loss = self.criterion(features, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Contrastive Loss: {total_loss / len(train_loader):.4f}')\n",
    "    \n",
    "    def train_classifier(self, train_loader, epochs):\n",
    "        self.encoder.eval()  # Freeze the encoder for classifier training\n",
    "        self.classifier.train()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Extract frozen features\n",
    "                with torch.no_grad():\n",
    "                    features = self.encoder(images)\n",
    "\n",
    "                # Forward pass through the classifier\n",
    "                outputs = self.classifier(features)\n",
    "                loss = F.cross_entropy(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            accuracy = correct / len(train_loader.dataset)\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Cross Entropy Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "   # def evaluate_embeddings(self, test_loader):\n",
    "    #    self.encoder.eval()\n",
    "   #     all_embeddings = []\n",
    "  #      all_labels = []\n",
    "#\n",
    "       # with torch.no_grad():\n",
    "       #       for images, labels in test_loader:\n",
    "       #            images, labels = images.to(device), labels.to(device)\n",
    "       #          features = self.encoder(images)\n",
    "       #          \n",
    "       #         all_embeddings.append(features.cpu())\n",
    "       #         all_labels.append(labels.cpu())\n",
    "       #\n",
    "       #all_embeddings = torch.cat(all_embeddings)\n",
    "       # all_labels = torch.cat(all_labels)\n",
    "\n",
    "       # print(accuracy_calculator.get_accuracy(all_embeddings, all_labels, all_embeddings, all_labels))\n",
    "\n",
    "    def test_classifier(self, test_loader, num_classes=10):\n",
    "        self.encoder.eval()\n",
    "        self.classifier.eval()\n",
    "        \n",
    "        # Variables to store predictions and true labels for AUC/ROC/Confusion Matrix\n",
    "        all_probs = []\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                # Get features and outputs\n",
    "                features = self.encoder(images)\n",
    "                outputs = self.classifier(features)\n",
    "                \n",
    "                # Convert logits to probabilities using softmax\n",
    "                probs = F.softmax(outputs, dim=1)\n",
    "                \n",
    "                # Record the true labels and predicted probabilities\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "                all_probs.append(probs.cpu().numpy())\n",
    "                \n",
    "                # Get predicted class from the output (class with max probability)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                all_preds.append(predicted.cpu().numpy())\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Flatten the collected lists\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        all_probs = np.concatenate(all_probs)\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        # Calculate test accuracy\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "        \n",
    "        # --- ROC and AUC computation ---\n",
    "        # One-hot encode the labels\n",
    "        labels_one_hot = np.eye(num_classes)[all_labels]\n",
    "\n",
    "        # Compute ROC and AUC for each class\n",
    "        roc_auc = {}\n",
    "        for i in range(num_classes):\n",
    "            fpr, tpr, _ = roc_curve(labels_one_hot[:, i], all_probs[:, i])\n",
    "            auc = roc_auc_score(labels_one_hot[:, i], all_probs[:, i])\n",
    "            roc_auc[f'Class {i}'] = auc\n",
    "            print(f'Class {i} AUC: {auc:.4f}')\n",
    "            \n",
    "        # Plot the ROC curve for each class\n",
    "        plt.plot(fpr, tpr, label=f'Class {i} (AUC = {auc:.2f})')\n",
    "        \n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()\n",
    "        \n",
    "        # Overall macro AUC score\n",
    "        overall_auc = roc_auc_score(labels_one_hot, all_probs, average='macro')\n",
    "        print(f'Overall AUC (macro): {overall_auc:.4f}')\n",
    "        \n",
    "        # --- Confusion Matrix ---\n",
    "        # Compute confusion matrix\n",
    "        conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=range(num_classes))\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with Optuna\n",
    "def objective(trial):\n",
    "\n",
    "    # Hyperparameters to tune\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 32, 128, step=32)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_gridsearch2_CNN_loader = DataLoader(train_gridsearch2_CNN, batch_size=batch_size, shuffle=True)\n",
    "    train_gridsearch2_MLP_loader = DataLoader(train_gridsearch2_MLP, batch_size=batch_size, shuffle=True)\n",
    "    validation_gridsearch_loader = DataLoader(validation_gridsearch, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    encoder = CNNEncoder().to(device)\n",
    "    classifier = MLPClassifier().to(device)\n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(classifier.parameters()), lr=lr)\n",
    "    contrastive_loss = SupConLoss().to(device)\n",
    "    \n",
    "    trainer = ContrastiveTrainer(encoder, classifier, contrastive_loss, optimizer)\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train_contrastive(train_gridsearch2_CNN_loader, epochs=5)  # Train for fewer epochs for tuning speed\n",
    "    trainer.train_classifier(train_gridsearch2_MLP_loader, epochs=3)\n",
    "    \n",
    "    # Evaluate on validation data\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_gridsearch_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            features = encoder(images)\n",
    "            outputs = classifier(features)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-21 16:23:11,618] A new study created in memory with name: Hyperparameter_Tunning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Contrastive Loss: 3.0116\n",
      "Epoch [2/5], Contrastive Loss: 2.5658\n",
      "Epoch [3/5], Contrastive Loss: 2.4621\n",
      "Epoch [4/5], Contrastive Loss: 2.4097\n",
      "Epoch [5/5], Contrastive Loss: 2.3675\n",
      "Epoch [1/3], Cross Entropy Loss: 2.2782, Accuracy: 17.52%\n",
      "Epoch [2/3], Cross Entropy Loss: 2.2160, Accuracy: 47.81%\n",
      "Epoch [3/3], Cross Entropy Loss: 2.1362, Accuracy: 73.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-21 16:36:50,522] Trial 0 finished with value: 0.8266666666666667 and parameters: {'lr': 0.0001300938332348746, 'batch_size': 96}. Best is trial 0 with value: 0.8266666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Contrastive Loss: 2.9071\n",
      "Epoch [2/5], Contrastive Loss: 2.4551\n",
      "Epoch [3/5], Contrastive Loss: 2.2738\n",
      "Epoch [4/5], Contrastive Loss: 2.1999\n",
      "Epoch [5/5], Contrastive Loss: 2.1420\n",
      "Epoch [1/3], Cross Entropy Loss: 2.2999, Accuracy: 6.06%\n",
      "Epoch [2/3], Cross Entropy Loss: 2.2895, Accuracy: 10.83%\n",
      "Epoch [3/3], Cross Entropy Loss: 2.2784, Accuracy: 14.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-21 16:50:46,753] Trial 1 finished with value: 0.16277777777777777 and parameters: {'lr': 1.964850149319003e-05, 'batch_size': 64}. Best is trial 0 with value: 0.8266666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Contrastive Loss: 2.9666\n",
      "Epoch [2/5], Contrastive Loss: 2.5211\n",
      "Epoch [3/5], Contrastive Loss: 2.4266\n",
      "Epoch [4/5], Contrastive Loss: 2.3777\n",
      "Epoch [5/5], Contrastive Loss: 2.3383\n",
      "Epoch [1/3], Cross Entropy Loss: 2.2725, Accuracy: 16.96%\n",
      "Epoch [2/3], Cross Entropy Loss: 2.1978, Accuracy: 46.93%\n",
      "Epoch [3/3], Cross Entropy Loss: 2.0933, Accuracy: 84.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-21 17:03:14,695] Trial 2 finished with value: 0.9288888888888889 and parameters: {'lr': 0.00016635635829091353, 'batch_size': 96}. Best is trial 2 with value: 0.9288888888888889.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Contrastive Loss: 2.9690\n",
      "Epoch [2/5], Contrastive Loss: 2.5123\n",
      "Epoch [3/5], Contrastive Loss: 2.4143\n",
      "Epoch [4/5], Contrastive Loss: 2.3702\n",
      "Epoch [5/5], Contrastive Loss: 2.3359\n",
      "Epoch [1/3], Cross Entropy Loss: 0.6437, Accuracy: 88.92%\n",
      "Epoch [2/3], Cross Entropy Loss: 0.1265, Accuracy: 96.48%\n",
      "Epoch [3/3], Cross Entropy Loss: 0.1146, Accuracy: 96.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-21 17:16:30,261] Trial 3 finished with value: 0.97 and parameters: {'lr': 0.008888571242581387, 'batch_size': 96}. Best is trial 3 with value: 0.97.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Contrastive Loss: 1.7777\n",
      "Epoch [2/5], Contrastive Loss: 1.3728\n",
      "Epoch [3/5], Contrastive Loss: 1.2829\n",
      "Epoch [4/5], Contrastive Loss: 1.2410\n",
      "Epoch [5/5], Contrastive Loss: 1.2069\n",
      "Epoch [1/3], Cross Entropy Loss: 2.2576, Accuracy: 29.39%\n",
      "Epoch [2/3], Cross Entropy Loss: 2.1115, Accuracy: 82.27%\n",
      "Epoch [3/3], Cross Entropy Loss: 1.8808, Accuracy: 93.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-21 17:35:33,082] Trial 4 finished with value: 0.9511111111111111 and parameters: {'lr': 9.972730860622994e-05, 'batch_size': 32}. Best is trial 3 with value: 0.97.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Contrastive Loss: 2.4414\n",
      "Epoch [2/5], Contrastive Loss: 2.0321\n",
      "Epoch [3/5], Contrastive Loss: 1.9409\n",
      "Epoch [4/5], Contrastive Loss: 1.8822\n",
      "Epoch [5/5], Contrastive Loss: 1.8469\n",
      "Epoch [1/3], Cross Entropy Loss: 2.1733, Accuracy: 44.65%\n",
      "Epoch [2/3], Cross Entropy Loss: 1.6733, Accuracy: 85.49%\n",
      "Epoch [3/3], Cross Entropy Loss: 1.0306, Accuracy: 94.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-21 17:49:38,163] Trial 5 finished with value: 0.9613888888888888 and parameters: {'lr': 0.0004597914293985484, 'batch_size': 64}. Best is trial 3 with value: 0.97.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Contrastive Loss: 3.2456\n",
      "Epoch [2/5], Contrastive Loss: 2.7724\n",
      "Epoch [3/5], Contrastive Loss: 2.6826\n",
      "Epoch [4/5], Contrastive Loss: 2.6317\n",
      "Epoch [5/5], Contrastive Loss: 2.5957\n",
      "Epoch [1/3], Cross Entropy Loss: 2.1040, Accuracy: 60.23%\n",
      "Epoch [2/3], Cross Entropy Loss: 1.4146, Accuracy: 82.69%\n",
      "Epoch [3/3], Cross Entropy Loss: 0.7658, Accuracy: 89.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-21 18:01:41,601] Trial 6 finished with value: 0.9019444444444444 and parameters: {'lr': 0.0011828897341217071, 'batch_size': 128}. Best is trial 3 with value: 0.97.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Contrastive Loss: 2.7840\n",
      "Epoch [2/5], Contrastive Loss: 2.3222\n",
      "Epoch [3/5], Contrastive Loss: 2.1702\n",
      "Epoch [4/5], Contrastive Loss: 2.1009\n",
      "Epoch [5/5], Contrastive Loss: 2.0574\n",
      "Epoch [1/3], Cross Entropy Loss: 2.3087, Accuracy: 10.03%\n",
      "Epoch [2/3], Cross Entropy Loss: 2.2912, Accuracy: 12.95%\n",
      "Epoch [3/3], Cross Entropy Loss: 2.2707, Accuracy: 18.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-21 18:11:46,853] Trial 7 finished with value: 0.25055555555555553 and parameters: {'lr': 3.505125024018691e-05, 'batch_size': 64}. Best is trial 3 with value: 0.97.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Contrastive Loss: 3.0494\n",
      "Epoch [2/5], Contrastive Loss: 2.5298\n",
      "Epoch [3/5], Contrastive Loss: 2.4220\n",
      "Epoch [4/5], Contrastive Loss: 2.3758\n",
      "Epoch [5/5], Contrastive Loss: 2.3371\n",
      "Epoch [1/3], Cross Entropy Loss: 0.6607, Accuracy: 91.44%\n",
      "Epoch [2/3], Cross Entropy Loss: 0.0774, Accuracy: 97.95%\n",
      "Epoch [3/3], Cross Entropy Loss: 0.0696, Accuracy: 98.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-21 18:20:56,574] Trial 8 finished with value: 0.9752777777777778 and parameters: {'lr': 0.007290442689557333, 'batch_size': 96}. Best is trial 8 with value: 0.9752777777777778.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Contrastive Loss: 3.3558\n",
      "Epoch [2/5], Contrastive Loss: 2.8978\n",
      "Epoch [3/5], Contrastive Loss: 2.7110\n",
      "Epoch [4/5], Contrastive Loss: 2.6275\n",
      "Epoch [5/5], Contrastive Loss: 2.5787\n",
      "Epoch [1/3], Cross Entropy Loss: 2.2941, Accuracy: 13.59%\n",
      "Epoch [2/3], Cross Entropy Loss: 2.2864, Accuracy: 21.77%\n",
      "Epoch [3/3], Cross Entropy Loss: 2.2779, Accuracy: 29.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-21 18:30:10,439] Trial 9 finished with value: 0.33805555555555555 and parameters: {'lr': 2.268177459051338e-05, 'batch_size': 96}. Best is trial 8 with value: 0.9752777777777778.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'lr': 0.007290442689557333, 'batch_size': 96}\n"
     ]
    }
   ],
   "source": [
    "# Run Optuna study\n",
    "study = optuna.create_study(study_name =\"Hyperparameter_Tunning\",direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best hyperparameters: \", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.007290442689557333, 'batch_size': 96}\n"
     ]
    }
   ],
   "source": [
    "# Use best hyperparameters to train the final model\n",
    "best_params = study.best_params\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rodri\\Desktop\\MEMEC - 2ºAno\\Sistemas Inteligentes\\Project\\SupervisedContrastiveLoss.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rodri/Desktop/MEMEC%20-%202%C2%BAAno/Sistemas%20Inteligentes/Project/SupervisedContrastiveLoss.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Use best hyperparameters to train the final model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rodri/Desktop/MEMEC%20-%202%C2%BAAno/Sistemas%20Inteligentes/Project/SupervisedContrastiveLoss.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_CNN_loader \u001b[39m=\u001b[39m DataLoader(train_CNN, batch_size\u001b[39m=\u001b[39mbest_params[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m], shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rodri/Desktop/MEMEC%20-%202%C2%BAAno/Sistemas%20Inteligentes/Project/SupervisedContrastiveLoss.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_MLP_loader \u001b[39m=\u001b[39m DataLoader(train_MLP, batch_size\u001b[39m=\u001b[39mbest_params[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m], shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rodri/Desktop/MEMEC%20-%202%C2%BAAno/Sistemas%20Inteligentes/Project/SupervisedContrastiveLoss.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m test_loader \u001b[39m=\u001b[39m DataLoader(test_data, batch_size\u001b[39m=\u001b[39mbest_params[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m], shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_params' is not defined"
     ]
    }
   ],
   "source": [
    "# Use best hyperparameters to train the final model\n",
    "train_CNN_loader = DataLoader(train_CNN, batch_size=best_params['batch_size'], shuffle=True)\n",
    "train_MLP_loader = DataLoader(train_MLP, batch_size=best_params['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "# \n",
    "encoder = CNNEncoder().to(device)\n",
    "classifier = MLPClassifier().to(device)\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(classifier.parameters()), lr=best_params['lr'])\n",
    "contrastive_loss = SupConLoss().to(device)\n",
    "\n",
    "\n",
    "trainer = ContrastiveTrainer(encoder, classifier, contrastive_loss, optimizer)\n",
    "\n",
    "# Final training and testing\n",
    "trainer.train_contrastive(train_CNN_loader, epochs=15)\n",
    "trainer.train_classifier(train_MLP_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rodri\\Desktop\\MEMEC - 2ºAno\\Sistemas Inteligentes\\Project\\SupervisedContrastiveLoss.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rodri/Desktop/MEMEC%20-%202%C2%BAAno/Sistemas%20Inteligentes/Project/SupervisedContrastiveLoss.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Test the classifier\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rodri/Desktop/MEMEC%20-%202%C2%BAAno/Sistemas%20Inteligentes/Project/SupervisedContrastiveLoss.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39mtest_classifier(test_loader)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "#Test the classifier\n",
    "trainer.test_classifier(test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1rc1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1rc1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98b9776bb1c906ffea5885633daef92fdfff9bdc53a036d784e355cfb10fec4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
