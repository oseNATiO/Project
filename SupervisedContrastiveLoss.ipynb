{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import All libraries \n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import optuna\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_metric_learning.losses import SupConLoss\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define image transformations (including normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load the full MNIST training set\n",
    "full_train_data = datasets.MNIST(root=\"D:\\MNIST\\MNIST_Train\", train=True, download=False, transform = transform)\n",
    "test_data = datasets.MNIST(root=\"D:\\MNIST\\MNIST_Test\", train=False, download=False, transform= transform)\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(42)  # You can choose any seed number\n",
    "\n",
    "# Define train-validation split sizes\n",
    "train_size = int(0.8 * len(full_train_data))  # 80% for training\n",
    "val_size = len(full_train_data) - train_size  # 20% for validation\n",
    "\n",
    "# Split the full training dataset\n",
    "train_data, val_data = random_split(full_train_data, [train_size, val_size])\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module): #Encoder as we're only interest in the final embedding provided by the CNN \n",
    "    def __init__(self):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.fc = nn.Linear(256 * 7 * 7, 128)  # Output: 128-dimension embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)  # 14x14\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)  # 7x7\n",
    "        \n",
    "        x = x.view(-1, 256 * 7 * 7)  # Flatten\n",
    "        x = self.fc(x)  # Output embeddings\n",
    "        return x\n",
    "\n",
    "class MLPClassifier(nn.Module): # MLP projection Head \n",
    "    def __init__(self):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize accuracy calculator\n",
    "#accuracy_calculator = AccuracyCalculator(include=(\"precision_at_1\",), knn_func=None)\n",
    "\n",
    "class ContrastiveTrainer:\n",
    "    def __init__(self, encoder, classifier, criterion, optimizer):\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train_contrastive(self, train_loader, epochs=10):\n",
    "        self.encoder.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass through the encoder\n",
    "                features = self.encoder(images)\n",
    "\n",
    "                # Compute supervised contrastive loss\n",
    "                loss = self.criterion(features, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Contrastive Loss: {total_loss / len(train_loader):.4f}')\n",
    "    \n",
    "    def train_classifier(self, train_loader, epochs=5):\n",
    "        self.encoder.eval()  # Freeze the encoder for classifier training\n",
    "        self.classifier.train()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Extract frozen features\n",
    "                with torch.no_grad():\n",
    "                    features = self.encoder(images)\n",
    "\n",
    "                # Forward pass through the classifier\n",
    "                outputs = self.classifier(features)\n",
    "                loss = F.cross_entropy(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            accuracy = correct / len(train_loader.dataset)\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Classification Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "   # def evaluate_embeddings(self, test_loader):\n",
    "    #    self.encoder.eval()\n",
    "   #     all_embeddings = []\n",
    "  #      all_labels = []\n",
    "#\n",
    "  #      with torch.no_grad():\n",
    "     #       for images, labels in test_loader:\n",
    "    #            images, labels = images.to(device), labels.to(device)\n",
    "      #          features = self.encoder(images)\n",
    "      #          \n",
    "       #         all_embeddings.append(features.cpu())\n",
    "       #         all_labels.append(labels.cpu())\n",
    "#\n",
    "        #all_embeddings = torch.cat(all_embeddings)\n",
    "       # all_labels = torch.cat(all_labels)\n",
    "\n",
    "       # print(accuracy_calculator.get_accuracy(all_embeddings, all_labels, all_embeddings, all_labels))\n",
    "\n",
    "    def test_classifier(self, test_loader):\n",
    "            self.encoder.eval()\n",
    "            self.classifier.eval()\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in test_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    features = self.encoder(images)\n",
    "                    outputs = self.classifier(features)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "            print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 32, 128, step=32)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    encoder = CNNEncoder().to(device)\n",
    "    classifier = MLPClassifier().to(device)\n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(classifier.parameters()), lr=lr)\n",
    "    contrastive_loss = SupConLoss().to(device)\n",
    "    \n",
    "    trainer = ContrastiveTrainer(encoder, classifier, contrastive_loss, optimizer)\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train_contrastive(train_loader, epochs=5)  # Train for fewer epochs for tuning speed\n",
    "    trainer.train_classifier(train_loader, epochs=3)\n",
    "    \n",
    "    # Evaluate on validation data\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            features = encoder(images)\n",
    "            outputs = classifier(features)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-15 15:24:23,035] A new study created in memory with name: no-name-83067d3d-f3c1-4beb-bdb2-f9228388e3f1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Contrastive Loss: 2.2075\n",
      "Epoch [2/5], Contrastive Loss: 1.9806\n",
      "Epoch [3/5], Contrastive Loss: 1.9242\n",
      "Epoch [4/5], Contrastive Loss: 1.8911\n",
      "Epoch [5/5], Contrastive Loss: 1.8701\n",
      "Epoch [1/3], Classification Loss: 2.2975, Accuracy: 9.90%\n",
      "Epoch [2/3], Classification Loss: 2.2809, Accuracy: 15.50%\n",
      "Epoch [3/3], Classification Loss: 2.2596, Accuracy: 32.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-15 16:26:42,989] Trial 0 finished with value: 0.36491666666666667 and parameters: {'lr': 1.9586116348528246e-05, 'batch_size': 64}. Best is trial 0 with value: 0.36491666666666667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Contrastive Loss: 2.1273\n",
      "Epoch [2/5], Contrastive Loss: 1.9237\n",
      "Epoch [3/5], Contrastive Loss: 1.8745\n",
      "Epoch [4/5], Contrastive Loss: 1.8475\n",
      "Epoch [5/5], Contrastive Loss: 1.8267\n",
      "Epoch [1/3], Classification Loss: 2.2882, Accuracy: 14.98%\n",
      "Epoch [2/3], Classification Loss: 2.2314, Accuracy: 32.24%\n",
      "Epoch [3/3], Classification Loss: 2.1375, Accuracy: 71.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-15 17:25:35,697] Trial 1 finished with value: 0.79225 and parameters: {'lr': 4.20183167702627e-05, 'batch_size': 64}. Best is trial 1 with value: 0.79225.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'lr': 4.20183167702627e-05, 'batch_size': 64}\n"
     ]
    }
   ],
   "source": [
    "# Run Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=2)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best hyperparameters: \", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Contrastive Loss: 2.1306\n",
      "Epoch [2/10], Contrastive Loss: 1.9258\n",
      "Epoch [3/10], Contrastive Loss: 1.8728\n",
      "Epoch [4/10], Contrastive Loss: 1.8443\n",
      "Epoch [5/10], Contrastive Loss: 1.8262\n",
      "Epoch [6/10], Contrastive Loss: 1.8112\n",
      "Epoch [7/10], Contrastive Loss: 1.8028\n",
      "Epoch [8/10], Contrastive Loss: 1.7948\n",
      "Epoch [9/10], Contrastive Loss: 1.7868\n",
      "Epoch [10/10], Contrastive Loss: 1.7810\n",
      "Epoch [1/5], Classification Loss: 2.2862, Accuracy: 10.58%\n",
      "Epoch [2/5], Classification Loss: 2.2260, Accuracy: 26.75%\n",
      "Epoch [3/5], Classification Loss: 2.1280, Accuracy: 64.84%\n",
      "Epoch [4/5], Classification Loss: 1.9979, Accuracy: 88.52%\n",
      "Epoch [5/5], Classification Loss: 1.8452, Accuracy: 96.93%\n",
      "Test Accuracy: 96.55%\n"
     ]
    }
   ],
   "source": [
    "# Use best hyperparameters to train the final model\n",
    "best_params = study.best_params\n",
    "train_loader = DataLoader(train_data, batch_size=best_params['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "# \n",
    "encoder = CNNEncoder().to(device)\n",
    "classifier = MLPClassifier().to(device)\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(classifier.parameters()), lr=best_params['lr'])\n",
    "contrastive_loss = SupConLoss().to(device)\n",
    "\n",
    "\n",
    "trainer = ContrastiveTrainer(encoder, classifier, contrastive_loss, optimizer)\n",
    "\n",
    "# Final training and testing\n",
    "trainer.train_contrastive(train_loader, epochs=10)\n",
    "trainer.train_classifier(train_loader, epochs=5)\n",
    "trainer.test_classifier(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x216 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(12, 3))  # Create a row of 5 images\n",
    "for i in range(5):\n",
    "    image, label = full_train_data[i]\n",
    "    axes[i].imshow(image.squeeze(), cmap='gray')\n",
    "    axes[i].set_title(f\"Label: {label}\")\n",
    "    axes[i].axis('off')  # Turn off the axes for clarity\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1rc1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1rc1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98b9776bb1c906ffea5885633daef92fdfff9bdc53a036d784e355cfb10fec4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
