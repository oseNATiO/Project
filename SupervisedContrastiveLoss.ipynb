{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st Approach : CNN + MLP (Pre-trained with Supervised Contrastive Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import All libraries \n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import optuna\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_metric_learning.losses import SupConLoss\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perfom the transformations necessary \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                     #Transform the image into a tensor \n",
    "    transforms.Normalize((0.1307,), (0.3081,)) #Normalize the data\n",
    "])\n",
    "\n",
    "# Load the MNIST Dataset\n",
    "full_train_data = datasets.MNIST(root=\"D:\\MNIST\\MNIST_Train\", train=True, download=False, transform = transform)\n",
    "test_data = datasets.MNIST(root=\"D:\\MNIST\\MNIST_Test\", train=False, download=False, transform= transform)\n",
    "\n",
    "# This seed is used to make sure the same split will happen for all approaches\n",
    "torch.manual_seed(42)  \n",
    "\n",
    "# Defining the sets that are going to be used for training and Gridsearch\n",
    "train_data = int(0.7 * len(full_train_data))  # 70% for training\n",
    "train_gridsearch = len(full_train_data) - train_data # 30% for gridsearch\n",
    "\n",
    "# Get the training sets \n",
    "train_data, train_gridsearch = random_split(full_train_data, [train_data, train_gridsearch])\n",
    "\n",
    "# Split the training datasets, one for the training of the CNN and one for the training of the MLP \n",
    "train_CNN = int(0.6 * len(train_data))\n",
    "train_MLP = len(train_data) - train_CNN\n",
    "train_CNN, train_MLP = random_split(train_data, [train_CNN, train_MLP])\n",
    "\n",
    "# Make the Training and Validation split of the train_gridsearch set\n",
    "train_gridsearch2 = int(0.8 * len(train_gridsearch))\n",
    "validation_gridsearch = len(train_gridsearch) - train_gridsearch2\n",
    "train_gridsearch2, validation_gridsearch = random_split(train_gridsearch, [train_gridsearch2, validation_gridsearch])\n",
    "\n",
    "# Make the split of the train_gridsearch2 for the CNN and the MLP\n",
    "train_gridsearch2_CNN = int(0.6 * len(train_gridsearch2))\n",
    "train_gridsearch2_MLP = len(train_gridsearch2) - train_gridsearch2_CNN\n",
    "train_gridsearch2_CNN, train_gridsearch2_MLP = random_split(train_gridsearch2, [train_gridsearch2_CNN, train_gridsearch2_MLP])\n",
    "\n",
    "# the device specifies where the tensor or model should be stored and computations should be performed\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module): #Encoder as we're only interest in the final embedding provided by the CNN \n",
    "    def __init__(self):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(256 * 7 * 7, 128)  # Output: 128-dimension embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)  # 14x14\n",
    "        \n",
    "        x = self.dropout1(F.relu(self.conv3(x))) #Adding Drop out\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)  # 7x7\n",
    "        \n",
    "        x = x.view(-1, 256 * 7 * 7)  # Flatten\n",
    "        x = self.fc(x)  # Output embeddings\n",
    "        \n",
    "         # Normalize embeddings to lie on unit hypersphere\n",
    "        x = F.normalize(x, p=2, dim=1)  # L2 normalization\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MLPClassifier(nn.Module): # MLP projection Head \n",
    "    def __init__(self):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.dropout2 = nn.Dropout(p=0.3)\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout2(F.relu(self.fc1(x))) #Adding Drop out\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize accuracy calculator\n",
    "#accuracy_calculator = AccuracyCalculator(include=(\"precision_at_1\",), knn_func=None)\n",
    "\n",
    "class ContrastiveTrainer:\n",
    "    def __init__(self, encoder, classifier, criterion, optimizer):\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train_contrastive(self, train_loader, epochs):\n",
    "        self.encoder.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass through the encoder\n",
    "                features = self.encoder(images)\n",
    "\n",
    "                # Compute supervised contrastive loss\n",
    "                loss = self.criterion(features, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Contrastive Loss: {total_loss / len(train_loader):.4f}')\n",
    "    \n",
    "    def train_classifier(self, train_loader, epochs):\n",
    "        self.encoder.eval()  # Freeze the encoder for classifier training\n",
    "        self.classifier.train()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Extract frozen features\n",
    "                with torch.no_grad():\n",
    "                    features = self.encoder(images)\n",
    "\n",
    "                # Forward pass through the classifier\n",
    "                outputs = self.classifier(features)\n",
    "                loss = F.cross_entropy(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            accuracy = correct / len(train_loader.dataset)\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Cross Entropy Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "   # def evaluate_embeddings(self, test_loader):\n",
    "    #    self.encoder.eval()\n",
    "   #     all_embeddings = []\n",
    "  #      all_labels = []\n",
    "#\n",
    "  #      with torch.no_grad():\n",
    "     #       for images, labels in test_loader:\n",
    "    #            images, labels = images.to(device), labels.to(device)\n",
    "      #          features = self.encoder(images)\n",
    "      #          \n",
    "       #         all_embeddings.append(features.cpu())\n",
    "       #         all_labels.append(labels.cpu())\n",
    "#\n",
    "        #all_embeddings = torch.cat(all_embeddings)\n",
    "       # all_labels = torch.cat(all_labels)\n",
    "\n",
    "       # print(accuracy_calculator.get_accuracy(all_embeddings, all_labels, all_embeddings, all_labels))\n",
    "\n",
    "    def test_classifier(self, test_loader, num_classes=10):\n",
    "        self.encoder.eval()\n",
    "        self.classifier.eval()\n",
    "        \n",
    "        # Variables to store predictions and true labels for AUC/ROC/Confusion Matrix\n",
    "        all_probs = []\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                # Get features and outputs\n",
    "                features = self.encoder(images)\n",
    "                outputs = self.classifier(features)\n",
    "                \n",
    "                # Convert logits to probabilities using softmax\n",
    "                probs = F.softmax(outputs, dim=1)\n",
    "                \n",
    "                # Record the true labels and predicted probabilities\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "                all_probs.append(probs.cpu().numpy())\n",
    "                \n",
    "                # Get predicted class from the output (class with max probability)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                all_preds.append(predicted.cpu().numpy())\n",
    "                \n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Flatten the collected lists\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        all_probs = np.concatenate(all_probs)\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        \n",
    "        # Calculate test accuracy\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "        \n",
    "        # --- ROC and AUC computation ---\n",
    "        # One-hot encode the labels\n",
    "        labels_one_hot = np.eye(num_classes)[all_labels]\n",
    "\n",
    "        # Compute ROC and AUC for each class\n",
    "        roc_auc = {}\n",
    "        for i in range(num_classes):\n",
    "            fpr, tpr, _ = roc_curve(labels_one_hot[:, i], all_probs[:, i])\n",
    "            auc = roc_auc_score(labels_one_hot[:, i], all_probs[:, i])\n",
    "            roc_auc[f'Class {i}'] = auc\n",
    "            print(f'Class {i} AUC: {auc:.4f}')\n",
    "            \n",
    "            # Plot the ROC curve for each class\n",
    "            plt.plot(fpr, tpr, label=f'Class {i} (AUC = {auc:.2f})')\n",
    "        \n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()\n",
    "        \n",
    "        # Overall macro AUC score\n",
    "        overall_auc = roc_auc_score(labels_one_hot, all_probs, average='macro')\n",
    "        print(f'Overall AUC (macro): {overall_auc:.4f}')\n",
    "        \n",
    "        # --- Confusion Matrix ---\n",
    "        # Compute confusion matrix\n",
    "        conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=range(num_classes))\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with Optuna\n",
    "def objective(trial):\n",
    "\n",
    "    # Hyperparameters to tune\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 32, 128, step=32)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_gridsearch2_CNN_loader = DataLoader(train_gridsearch2_CNN, batch_size=batch_size, shuffle=True)\n",
    "    train_gridsearch2_MLP_loader = DataLoader(train_gridsearch2_MLP, batch_size=batch_size, shuffle=True)\n",
    "    validation_gridsearch_loader = DataLoader(validation_gridsearch, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    encoder = CNNEncoder().to(device)\n",
    "    classifier = MLPClassifier().to(device)\n",
    "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(classifier.parameters()), lr=lr)\n",
    "    contrastive_loss = SupConLoss().to(device)\n",
    "    \n",
    "    trainer = ContrastiveTrainer(encoder, classifier, contrastive_loss, optimizer)\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train_contrastive(train_gridsearch2_CNN_loader, epochs=5)  # Train for fewer epochs for tuning speed\n",
    "    trainer.train_classifier(train_gridsearch2_MLP_loader, epochs=3)\n",
    "    \n",
    "    # Evaluate on validation data\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_gridsearch_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            features = encoder(images)\n",
    "            outputs = classifier(features)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-19 16:47:09,042] A new study created in memory with name: Hyperparameter_Tunning\n",
      "[W 2024-10-19 16:48:03,492] Trial 0 failed with parameters: {'lr': 0.0003848940927731642, 'batch_size': 32} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\rodri\\AppData\\Local\\Temp\\ipykernel_10772\\2614374942.py\", line 22, in objective\n",
      "    trainer.train_contrastive(train_gridsearch2_CNN_loader, epochs=5)  # Train for fewer epochs for tuning speed\n",
      "  File \"C:\\Users\\rodri\\AppData\\Local\\Temp\\ipykernel_10772\\3735225663.py\", line 25, in train_contrastive\n",
      "    self.optimizer.step()\n",
      "  File \"c:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\optimizer.py\", line 140, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"c:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\optimizer.py\", line 23, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"c:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\adam.py\", line 234, in step\n",
      "    adam(params_with_grad,\n",
      "  File \"c:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\adam.py\", line 300, in adam\n",
      "    func(params,\n",
      "  File \"c:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\adam.py\", line 363, in _single_tensor_adam\n",
      "    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
      "KeyboardInterrupt\n",
      "[W 2024-10-19 16:48:03,604] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rodri\\Desktop\\MEMEC - 2ºAno\\Sistemas Inteligentes\\Project\\SupervisedContrastiveLoss.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rodri/Desktop/MEMEC%20-%202%C2%BAAno/Sistemas%20Inteligentes/Project/SupervisedContrastiveLoss.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Run Optuna study\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rodri/Desktop/MEMEC%20-%202%C2%BAAno/Sistemas%20Inteligentes/Project/SupervisedContrastiveLoss.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(study_name \u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHyperparameter_Tunning\u001b[39m\u001b[39m\"\u001b[39m,direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rodri/Desktop/MEMEC%20-%202%C2%BAAno/Sistemas%20Inteligentes/Project/SupervisedContrastiveLoss.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rodri/Desktop/MEMEC%20-%202%C2%BAAno/Sistemas%20Inteligentes/Project/SupervisedContrastiveLoss.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Best hyperparameters\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rodri/Desktop/MEMEC%20-%202%C2%BAAno/Sistemas%20Inteligentes/Project/SupervisedContrastiveLoss.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest hyperparameters: \u001b[39m\u001b[39m\"\u001b[39m, study\u001b[39m.\u001b[39mbest_params)\n",
      "File \u001b[1;32mc:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m     \u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     _optimize(\n\u001b[0;32m    476\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    477\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    478\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    479\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    480\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    481\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    482\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    483\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    484\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    485\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         _optimize_sequential(\n\u001b[0;32m     64\u001b[0m             study,\n\u001b[0;32m     65\u001b[0m             func,\n\u001b[0;32m     66\u001b[0m             n_trials,\n\u001b[0;32m     67\u001b[0m             timeout,\n\u001b[0;32m     68\u001b[0m             catch,\n\u001b[0;32m     69\u001b[0m             callbacks,\n\u001b[0;32m     70\u001b[0m             gc_after_trial,\n\u001b[0;32m     71\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     72\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     73\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     74\u001b[0m         )\n\u001b[0;32m     75\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    198\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32mc:\\Users\\rodri\\Desktop\\MEMEC - 2ºAno\\Sistemas Inteligentes\\Project\\SupervisedContrastiveLoss.ipynb Cell 7\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rodri/Desktop/MEMEC%20-%202%C2%BAAno/Sistemas%20Inteligentes/Project/SupervisedContrastiveLoss.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m trainer \u001b[39m=\u001b[39m ContrastiveTrainer(encoder, classifier, contrastive_loss, optimizer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rodri/Desktop/MEMEC%20-%202%C2%BAAno/Sistemas%20Inteligentes/Project/SupervisedContrastiveLoss.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rodri/Desktop/MEMEC%20-%202%C2%BAAno/Sistemas%20Inteligentes/Project/SupervisedContrastiveLoss.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain_contrastive(train_gridsearch2_CNN_loader, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)  \u001b[39m# Train for fewer epochs for tuning speed\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rodri/Desktop/MEMEC%20-%202%C2%BAAno/Sistemas%20Inteligentes/Project/SupervisedContrastiveLoss.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain_classifier(train_gridsearch2_MLP_loader, epochs\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rodri/Desktop/MEMEC%20-%202%C2%BAAno/Sistemas%20Inteligentes/Project/SupervisedContrastiveLoss.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Evaluate on validation data\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\rodri\\Desktop\\MEMEC - 2ºAno\\Sistemas Inteligentes\\Project\\SupervisedContrastiveLoss.ipynb Cell 7\u001b[0m in \u001b[0;36mContrastiveTrainer.train_contrastive\u001b[1;34m(self, train_loader, epochs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rodri/Desktop/MEMEC%20-%202%C2%BAAno/Sistemas%20Inteligentes/Project/SupervisedContrastiveLoss.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(features, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rodri/Desktop/MEMEC%20-%202%C2%BAAno/Sistemas%20Inteligentes/Project/SupervisedContrastiveLoss.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rodri/Desktop/MEMEC%20-%202%C2%BAAno/Sistemas%20Inteligentes/Project/SupervisedContrastiveLoss.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rodri/Desktop/MEMEC%20-%202%C2%BAAno/Sistemas%20Inteligentes/Project/SupervisedContrastiveLoss.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rodri/Desktop/MEMEC%20-%202%C2%BAAno/Sistemas%20Inteligentes/Project/SupervisedContrastiveLoss.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m], Contrastive Loss: \u001b[39m\u001b[39m{\u001b[39;00mtotal_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    235\u001b[0m          grads,\n\u001b[0;32m    236\u001b[0m          exp_avgs,\n\u001b[0;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    239\u001b[0m          state_steps,\n\u001b[0;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[0;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 300\u001b[0m func(params,\n\u001b[0;32m    301\u001b[0m      grads,\n\u001b[0;32m    302\u001b[0m      exp_avgs,\n\u001b[0;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    305\u001b[0m      state_steps,\n\u001b[0;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\rodri\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\adam.py:363\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    360\u001b[0m     param \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(param)\n\u001b[0;32m    362\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39;49madd_(grad, alpha\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta1)\n\u001b[0;32m    364\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m    366\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run Optuna study\n",
    "study = optuna.create_study(study_name =\"Hyperparameter_Tunning\",direction='maximize')\n",
    "study.optimize(objective, n_trials=4)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best hyperparameters: \", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best hyperparameters to train the final model\n",
    "best_params = study.best_params\n",
    "train_CNN_loader = DataLoader(train_CNN, batch_size=best_params['batch_size'], shuffle=True)\n",
    "train_MLP_loader = DataLoader(train_MLP, batch_size=best_params['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "# \n",
    "encoder = CNNEncoder().to(device)\n",
    "classifier = MLPClassifier().to(device)\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(classifier.parameters()), lr=best_params['lr'])\n",
    "contrastive_loss = SupConLoss().to(device)\n",
    "\n",
    "\n",
    "trainer = ContrastiveTrainer(encoder, classifier, contrastive_loss, optimizer)\n",
    "\n",
    "# Final training and testing\n",
    "trainer.train_contrastive(train_CNN_loader, epochs=10)\n",
    "trainer.train_classifier(train_MLP_loader, epochs=5)\n",
    "trainer.test_classifier(test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1rc1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1rc1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98b9776bb1c906ffea5885633daef92fdfff9bdc53a036d784e355cfb10fec4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
